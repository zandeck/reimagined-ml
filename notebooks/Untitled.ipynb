{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2b3e2f6-10f6-4e69-84b8-962308e17881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd58efa-6057-480f-a11c-01296602c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'../data/input.txt', 'r', encoding='utf-8') as file:\n",
    "    # data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d2b287-d468-4654-8282-c69e2eb0211a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab = sorted(list(set(data)))\n",
    "# print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d77431-504f-4ed7-bf45-115eb3f3b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the tokenization here: simple digil encoding here\n",
    "# ctoi = { c: i for i, c in enumerate(vocab) }\n",
    "# itoc = { i: c for c, i in ctoi.items() }\n",
    "# encode = lambda s: [ctoi[c] for c in s] \n",
    "# decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "# tokenized_data = [ ctoi[c] for c in data ]\n",
    "# tokenized_data = torch.tensor(tokenized_data, dtype=torch.long)\n",
    "# print(tokenized_data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08df4a3c-d4ae-430c-a40c-9c79325b409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDS(Dataset):\n",
    "    def __init__(self, location, window_size):\n",
    "        self.location = location\n",
    "        self.window_size = window_size\n",
    "        print(self.location, self.window_size)\n",
    "        print('Load Dataset')\n",
    "        \n",
    "        with open(self.location, 'r', encoding='utf-8') as file:\n",
    "            self.data = file.read()\n",
    "\n",
    "        print('Loading done')        \n",
    "        self.vocab = sorted(list(set(data)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.ctoi = { c: i for i, c in enumerate(self.vocab) }\n",
    "        self.itoc = { i: c for c, i in self.ctoi.items() }\n",
    "\n",
    "        self.data = [ self.ctoi[c] for c in self.data ]\n",
    "        self.data = torch.tensor(self.data, dtype=torch.long)\n",
    "        print('Init Dataset Done')\n",
    "            \n",
    "    def encode(self, s):\n",
    "        return [self.ctoi[c] for c in s]\n",
    "\n",
    "    def decode(self, l):\n",
    "        return ''.join([self.itoc[i] for i in l])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.window_size]\n",
    "        y = self.data[idx+1:idx+self.window_size+1]\n",
    "\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ce13702-125b-466f-8712-a41f2a04ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We prepare the dataset here:\n",
    "# pct_split = 0.9\n",
    "# idx_split = int(pct_split * len(data))\n",
    "# train_data = tokenized_data[:idx_split]\n",
    "# test_data = tokenized_data[idx_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec390728-9e40-459d-9fb1-400c39a31e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4\n",
    "# context_window = 8\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# def batch(set_type, batch_size, context_window, device):\n",
    "#     ds = train_data if set_type == 'train' else test_data\n",
    "#     samples = torch.randint(0, len(ds) - context_window, (batch_size, ))\n",
    "#     x = torch.stack([ds[idx:idx+context_window] for idx in samples])\n",
    "#     y = torch.stack([ds[idx+1:idx+context_window+1] for idx in samples])\n",
    "#     return x.to(device), y.to(device)\n",
    "\n",
    "# x, y = batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9bb9857e-6178-4859-8e0c-dbe3b83544c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, n_embedding, head_size, context_window):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(context_window, context_window)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        key = self.key(x) \n",
    "        query = self.query(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        dot_product = query @ key.transpose(-2, -1) * (C ** -.5)\n",
    "        dot_product = dot_product.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        softmax = F.softmax(dot_product, dim=-1)\n",
    "        out = softmax @ value\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bb8a2ea-0d81-405b-a3cb-f6be068adea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, n_heads, n_embedding, head_size, context_window):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(n_embedding, head_size, context_window) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(n_heads * head_size, n_heads * head_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.concat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88f85147-ba3f-41de-9b68-5e5d5d990ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, s_input, s_intermediate):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(s_input, s_intermediate, bias=True)\n",
    "        self.linear2 = nn.Linear(s_intermediate, s_input, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45c1a320-6f48-408c-9908-d8ca660ebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, n_embedding, context_window):\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_heads\n",
    "        self.attention_layer = MultiAttentionHead(n_heads, n_embedding, head_size, context_window)\n",
    "        self.ffw = FeedForward(head_size * n_heads, 4 * n_embedding)\n",
    "        self.ln1 = nn.LayerNorm(head_size * n_heads)\n",
    "        self.ln2 = nn.LayerNorm(head_size * n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.attention_layer(self.ln1(x))\n",
    "        out = out + self.ffw(self.ln2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93e11b93-eef7-4735-af98-8445951ee6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_vocab, n_blocks, context_size, n_embedding, n_heads):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_embedding)\n",
    "        self.positional_embedding = nn.Embedding(context_size, n_embedding)\n",
    "        self.blocks = nn.Sequential(*[Block(n_heads, n_embedding, context_size) for _ in range(n_blocks)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embedding)\n",
    "        self.layer = nn.Linear(n_embedding, n_vocab)\n",
    "\n",
    "    def forward(self, device, x, y=None):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T, device=device))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer(x)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = x.shape\n",
    "            x = x.view(B*T, C)\n",
    "            targets = y.view(B*T)\n",
    "            loss = F.cross_entropy(x, targets)\n",
    "\n",
    "        return x, loss\n",
    "        \n",
    "    def generate(self, device, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.context_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(device, idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5999837f-ab5d-46d2-a174-3010adffd0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/input.txt 32\n",
      "Load Dataset\n",
      "Loading done\n",
      "Init Dataset Done\n"
     ]
    }
   ],
   "source": [
    "context_window = 32\n",
    "batch_size = 64\n",
    "\n",
    "dataset = TextDS(\"../data/input.txt\", context_window)\n",
    "train_set, test_set = random_split(dataset, [0.98, 0.02])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b092ecde-362b-4dc6-bb1d-94981e8cc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    # for split in ['train', 'val']:\n",
    "    losses = torch.zeros(len(test_dataloader))\n",
    "\n",
    "    for i, (x, y) in enumerate(test_dataloader):\n",
    "        logits, loss = model(device, x, y)\n",
    "        losses[i] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "        \n",
    "    #     for k in range(eval_iters):\n",
    "    #         X, Y = batch(split, batch_size, context_window, device)\n",
    "    #         logits, loss = model(device, X, Y)\n",
    "    #         losses[k] = loss.item()\n",
    "    #     out[split] = losses.mean()\n",
    "    # model.train()\n",
    "    # return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe89be3-4565-4d7f-b9b6-2b3d6ad097fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: val loss 4.4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Error 1.4498: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17079/17079 [10:55<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: val loss 1.4049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Error 1.3556: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17079/17079 [12:09<00:00, 23.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2: val loss 1.3551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Error 1.3516: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17079/17079 [10:35<00:00, 26.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: val loss 1.3274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Error 1.2805: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17079/17079 [11:20<00:00, 25.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: val loss 1.3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Error 1.2672:  27%|██████████████████████████████▉                                                                                  | 4678/17079 [03:08<08:29, 24.35it/s]"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "context_window = 32\n",
    "# batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")  \n",
    "model = Model(n_vocab=dataset.vocab_size, n_blocks=4, context_size=context_window, n_embedding=64, n_heads=4)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "max_iters = 5000\n",
    "max_epoch = 10\n",
    "eval_interval = 100\n",
    "\n",
    "\n",
    "for i in range(max_epoch):\n",
    "    losses = estimate_loss(model, device)\n",
    "    print(f\"step {i}: val loss {losses:.4f}\")\n",
    "    \n",
    "    for i, (x, y) in enumerate(pbar:=tqdm(train_dataloader)):\n",
    "        \n",
    "        \n",
    "    # if i % eval_interval == 0 or i == max_iters - 1:\n",
    "    #     losses = estimate_loss(model, batch_size, context_window, device)\n",
    "    #     print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    # xb, yb = batch('train', batch_size, context_window, device)\n",
    "\n",
    "    # evaluate the loss\n",
    "        logits, loss = model(device, x, y)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            pbar.set_description(f\"Training Error {loss:.4f}\")\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549cf2e-7a5a-41a6-8aee-8e30e752fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(device, context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "480ff4f1-b584-4778-8af3-9cf793f114ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tarly. They were fut.\n",
      "<|endoftext|>\n",
      "Once upon a time there was a fun share. So he meates to explore. \n",
      "The miserah went to try for to play with it him.\n",
      "Mitten try to your boy, showed whiled Lon't loak i\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(device, context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e230b-f04e-415b-b6db-6bd60ae69780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213ff2b-61b3-43c2-8089-de2069d4aab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
